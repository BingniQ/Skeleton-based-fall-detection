{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thesis_train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BingniQ/Skeleton-based-fall-detection/blob/master/High-quality%20simulation%20dataset-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2NHA8FIlrVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "542450bf-686d-4efa-c01f-7b3c3cf95381"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130824 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XThkK1rkmFqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e0441776-c23c-4fd7-9611-329f8abbb9eb"
      },
      "source": [
        "from google.colab import drive\n",
        "!mkdir drive\n",
        "!google-drive-ocamlfuse drive\n",
        "!ls drive/\"Colab Notebooks/thesis/view_seq_1_10\"\n",
        "#f = open(\"drive/Colab Notebooks/data/subj_seq/new_file_list_test.txt\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new_array_list_test.h5\t new_file_list_test.txt\n",
            "new_array_list_train.h5  new_file_list_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmqrR0-KmRV1",
        "colab_type": "code",
        "outputId": "7d50c0ca-35f8-4b5d-fa2b-464d775444fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import scipy\n",
        "from scipy import linalg\n",
        "import random\n",
        "import math\n",
        "import h5py\n",
        "import theano\n",
        "import keras\n",
        "from theano import tensor as T\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, Bidirectional, \\\n",
        "    GRU, SimpleRNN, Input, SpatialDropout1D, Reshape, Permute, merge, Lambda\n",
        "from keras.layers.merge import Add, Concatenate, Maximum\n",
        "from keras.layers.convolutional import Convolution2D,Convolution3D\n",
        "from keras.layers.pooling import MaxPooling1D\n",
        "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.optimizers import RMSprop,SGD,Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras.regularizers import l2, l1\n",
        "from keras.constraints import maxnorm, unitnorm\n",
        "import tensorflow as tf\n",
        "from statistics import mean\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEQUPtKLmR8s",
        "colab_type": "code",
        "outputId": "c9240556-0ca1-4800-ed43-53afaf329224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11041
        }
      },
      "source": [
        "def init_mean1(shape, dtype=None, name=None):\n",
        "    value = np.array([0,0,-1.0/4,0,-1.0/4,  0,0,0,-1.0/4,0,  0,0,0,0,0,  0,0,0,0,0,  -1.0/4,0,0,0,0 ])\n",
        "    value = np.reshape(value, shape)\n",
        "    return value\n",
        "\n",
        "def rand_rotate_matrix(angle1=-90, angle2=90, s1=0.5, s2=1.5):\n",
        "    random.random()\n",
        "    agx = random.uniform(angle1, angle2)  # do not use randint\n",
        "    agy = random.uniform(angle1, angle2)\n",
        "    s = random.uniform(s1, s2)\n",
        "    agx = math.radians(agx)\n",
        "    agy = math.radians(agy)\n",
        "    Rx = np.asarray([[math.cos(agx),math.sin(agx)], [-math.sin(agx),math.cos(agx)]])\n",
        "    Ry = np.asarray([[math.cos(agy), -math.sin(agy)], [math.sin(agy),  math.cos(agy)]])\n",
        "    value = np.dot(Ry,Rx)\n",
        "    # value = np.reshape(value, shape)\n",
        "    value=tf.convert_to_tensor(value)\n",
        "    # value = torch.from_numpy(value)\n",
        "    return value\n",
        "\n",
        "class TransformLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TransformLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        new_x=tf.convert_to_tensor(x)\n",
        "        new_y=tf.convert_to_tensor(rand_rotate_matrix())\n",
        "        return K.in_train_phase(K.dot(x,tf.cast(rand_rotate_matrix(),tf.float32) ), x, training=training)\n",
        "        # return K.in_train_phase(T.concatenate([K.dot(x[:,:,:,0:3], rand_rotate_matrix_symbol()), x[:,:,:,3:6] ], axis=3), x, training=training)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "class construct_model(object):\n",
        "    def __init__(self, param, dim_point=2, num_joints=18, num_class=2):\n",
        "        self._param = param\n",
        "        self._dim_point = dim_point\n",
        "        self._num_joints = num_joints\n",
        "        self._num_class = num_class\n",
        "\n",
        "    def group_person_list(self, list_file):\n",
        "        name_list = [line.strip() for line in open(list_file, 'r').readlines()]\n",
        "        vdname_list = [line[0:line.index('_P')] for line in name_list ]\n",
        "        label_list = [(int(name.split('_')[4][1])) for name in name_list]\n",
        "        idx_per = []\n",
        "        group_list = []\n",
        "        for idx, name in enumerate(name_list):\n",
        "            vdname = vdname_list[idx]\n",
        "            if idx == len(name_list)-1:\n",
        "                last_vdname = ''\n",
        "            else:\n",
        "                last_vdname = vdname_list[idx+1]\n",
        "            if vdname != last_vdname:\n",
        "                idx_per.append(idx)\n",
        "                # there exist samples with 3 skeletons, to check standard deviation\n",
        "                # print (len(idx_per), idx_per) 1 [10] 1 [11] 2 [12, 13]\n",
        "                group_list.append(idx_per)\n",
        "                # print [label_list[temp] for temp in idx_per]\n",
        "                idx_per = []\n",
        "            else:\n",
        "                idx_per.append(idx)\n",
        "#         print(group_list) # [[0]-[2315]]\n",
        "        return group_list\n",
        "\n",
        "    def spatial_diff(self, skeleton):\n",
        "        assert(skeleton.shape[2] == 2), ' input must be skeleton array'\n",
        "        fidx = [ 8,5,    2, 6,7,8,   2,3,4,5,   2,12,13,14,   2, 9,10,11]\n",
        "        assert(len(fidx) == skeleton.shape[1] )\n",
        "        return skeleton[:,np.array(fidx)-1 ] - skeleton\n",
        "        # return np.concatenate((skeleton, skeleton[:,np.array(fidx)-1 ] - skeleton ), axis=-1)\n",
        "\n",
        "    def spatial_cross(self, skeleton):\n",
        "        assert(skeleton.shape[2] == 2), ' input must be skeleton array'\n",
        "        fidx1 = [17,21,4,21,  6,5,6,22,  21,11,12,24,  1,13,16,14,  18,17,18,19,  5,8,8,  12,12]\n",
        "        fidx2 = [13,1,21,3,  21,7,8,23,  10,9,10,25,  14,15,14,15,  1,19,20,18,  9,23,22, 25,24]\n",
        "        skt1 = skeleton[:,np.array(fidx1)-1 ] - skeleton\n",
        "        skt2 = skeleton[:,np.array(fidx2)-1 ] - skeleton\n",
        "        return 100*np.cross(skt1, skt2)\n",
        "\n",
        "    def load_sample_one_skeleton(self, h5_file, list_file, num_seq=100, ovr_num=100, spatil_diff=True ):\n",
        "        '''\n",
        "        To change overlap number\n",
        "        '''\n",
        "        name_list = [line.strip() for line in open(list_file, 'r').readlines()]\n",
        "        vdname_list = [line[0:line.index('_P')] for line in name_list ]\n",
        "        label_list = [(int(name.split('_')[4][1])) for name in name_list]\n",
        "        new_label_list=[]\n",
        "        fall_number=0\n",
        "        fall_number2=0\n",
        "        for i in name_list:\n",
        "            if \"A0\" in i:\n",
        "                fall_number2=fall_number2+1\n",
        "\n",
        "        for label in label_list:\n",
        "            if label == 0:\n",
        "                new_label_list.append(1)\n",
        "                fall_number=fall_number+1\n",
        "            else:\n",
        "                new_label_list.append(0)\n",
        "\n",
        "        print(\"load fall number\",fall_number)\n",
        "        print(\"load fall number from file\",fall_number2)\n",
        "        print(\"load total number\",len(name_list))\n",
        "        # print(label_list) # [32, 5, 42, 2, 3, 30, 39, 36, 42, 18, 20, 42, 35, 35,\n",
        "        # print(vdname_list) # 'S006C002P007R001A033', 'S006C001P007R002A006', 'S002C003P007R001A043', 'S012C002P037R002A003', 'S002C001P011R002A004', 'S007C001P007R002A031', 'S007C003P007R002A040', 'S005C001P010R001A037', 'S008C001P032R002A043', 'S014C002P039R002A019', 'S004C001P007R001A021', 'S006C002P007R002A043', 'S016C003P040R002A036', 'S016C003P040R002A036',\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "        vid_list = []\n",
        "        with h5py.File(h5_file,'r') as hf:\n",
        "            group_list = self.group_person_list(list_file)\n",
        "            for idx_per in group_list:\n",
        "                # labels in list are the same\n",
        "                label_per = new_label_list[idx_per[0]]\n",
        "                vdname = vdname_list[idx_per[0]]\n",
        "                for idx in idx_per:\n",
        "                    skeleton = np.asarray(hf.get(name_list[idx]))\n",
        "\n",
        "                    if spatil_diff:\n",
        "                        skeleton = self.spatial_diff(skeleton)\n",
        "                        # skeleton = self.spatial_cross(skeleton)\n",
        "\n",
        "                    # print(skeleton.shape) #(60, 25, 3) (73, 25, 3) (60, 25, 3) (103, 25, 3) (96, 25, 3) (71, 25, 3) (48, 25, 3) (118, 25, 3) (97, 25, 3) (68, 25, 3) (84, 25, 3) (72, 25, 3) (68, 25, 3) (68, 25, 3) (63, 25, 3)\n",
        "                    if skeleton.shape[0] >= num_seq:\n",
        "                        start = 0\n",
        "                        while start + num_seq < skeleton.shape[0]:\n",
        "                            X.append(skeleton[start:start+num_seq])  # 0-100 100-200\n",
        "                            Y.append(label_per)\n",
        "                            vid_list.append(vdname)\n",
        "                            start = start + ovr_num\n",
        "                            # print((skeleton[start:start+num_seq]).shape) (3, 25, 3) (18, 25, 3)\n",
        "                        X.append(skeleton[-num_seq:]) # 为什么？？\n",
        "                        # print((skeleton[-num_seq:]).shape) (100, 25, 3) (100, 25, 3)\n",
        "                        Y.append(label_per)\n",
        "                        vid_list.append(vdname)\n",
        "                    else:\n",
        "                        skeleton = np.concatenate((np.zeros((num_seq-skeleton.shape[0], skeleton.shape[1], skeleton.shape[2])), skeleton), axis=0)\n",
        "                        # print(skeleton.shape) (100, 25, 3) 补零\n",
        "                        X.append(skeleton)\n",
        "                        Y.append(label_per)\n",
        "                        vid_list.append(vdname)\n",
        "        X = np.asarray(X).astype(np.float32)\n",
        "        Y = (np.asarray(Y)).astype(np.int32)\n",
        "        return X, Y, vid_list\n",
        "\n",
        "    def base_model(self, sub_mean=False, rotate=True):\n",
        "        '''\n",
        "        use stacked two layers as baseline, use stacked three layers later\n",
        "        K.learning_phase()\n",
        "        assert(self._dim_point == 3)\n",
        "        data = Dense(self._dim_point, kernel_initializer=rand_rotate_matrix, trainable=False)(skt_input)\n",
        "        '''\n",
        "        skt_input = Input(shape=(self._param['num_seq'], self._num_joints, self._dim_point) ) # To fix length of sequence\n",
        "        data = skt_input\n",
        "        if rotate:\n",
        "            if self._dim_point == 2:\n",
        "                data = TransformLayer()(skt_input)\n",
        "            else:\n",
        "                data = Reshape((self._param['num_seq'], int(self._num_joints*self._dim_point/2),2))(skt_input)\n",
        "                data = TransformLayer()(data)\n",
        "                data = Reshape((self._param['num_seq'], self._num_joints, self._dim_point))(data)\n",
        "\n",
        "        if sub_mean:\n",
        "            data = Permute((1,3,2))(data)\n",
        "            data2 = Dense(1, kernel_initializer=init_mean1, trainable=False)(data)\n",
        "            data2 = Lambda(lambda x:K.repeat_elements(x, self._num_joints, axis=-1),\n",
        "                           output_shape=lambda s: (s[0], s[1], s[2], s[3]*self._num_joints))(data2)\n",
        "            data = Add()([data, data2] )\n",
        "\n",
        "        # make sure do not subtract two mean vectors and concatenate the results\n",
        "        data = Reshape((self._param['num_seq'], self._num_joints*self._dim_point))(data)\n",
        "\n",
        "        data = SpatialDropout1D(0.05)(data)\n",
        "        out = Bidirectional(LSTM(512, return_sequences=True))(data)\n",
        "        out = SpatialDropout1D(0.05)(out)\n",
        "        out = Bidirectional(LSTM(512, return_sequences=True))(out)\n",
        "        out = SpatialDropout1D(0.05)(out)\n",
        "        out = Bidirectional(LSTM(512, return_sequences=True))(out)\n",
        "#         out = SpatialDropout1D(0.05)(out)\n",
        "#         out = Bidirectional(LSTM(512, return_sequences=True))(out)\n",
        "        # 把T.max改成了 K.max\n",
        "        out = Lambda(lambda x:K.max(x, axis=1), output_shape=lambda s: (s[0], s[2]))(out)\n",
        "        out = Dropout(0.5)(out)\n",
        "        out = Activation('relu')(out)\n",
        "        prob = Dense(self._num_class, activation='softmax')(out)\n",
        "\n",
        "        model = Model(skt_input, prob)\n",
        "        opt = SGD(lr=self._param['base_learn_rate'], decay=self._param['weight_regular'], momentum=0.9, nesterov=True)\n",
        "        model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def train_model(self):\n",
        "        model = self.base_model()\n",
        "        # test\n",
        "        valX, valY, val_vid_list = self.load_sample_one_skeleton(self._param['tst_arr_file'], self._param['tst_lst_file'],\n",
        "                                                                 self._param['num_seq'] ) # self._param['tst_angle_file'],\n",
        "        # train\n",
        "        trainX, trainY, train_vid_list = self.load_sample_one_skeleton(self._param['trn_arr_file'], self._param['trn_lst_file'],\n",
        "                                                                       self._param['num_seq'] )\n",
        "        test_fall=0\n",
        "        train_fall=0\n",
        "        for i in valY:\n",
        "            if i==1:\n",
        "                test_fall=test_fall+1\n",
        "\n",
        "        for j in trainY:\n",
        "            if j==0:\n",
        "                train_fall=train_fall+1\n",
        "\n",
        "        trainY = np_utils.to_categorical(trainY, self._num_class )\n",
        "        valY = np_utils.to_categorical(valY, self._num_class )\n",
        "\n",
        "\n",
        "        print( 'train data:', trainX.shape, trainY.shape)\n",
        "        print( 'test data:', valX.shape, valY.shape)\n",
        "        print(\"train fall data\", train_fall)\n",
        "        print(\"test fall data\", test_fall)\n",
        "\n",
        "        def save_hdf5(model, fileName):\n",
        "            fid = h5py.File(fileName,'w')\n",
        "            weight = model.get_weights()\n",
        "            for i in range(len(weight)):\n",
        "                fid.create_dataset('weight'+str(i),data=weight[i])\n",
        "            fid.close()\n",
        "\n",
        "        def read_hdf5(model, fileName):\n",
        "            fid=h5py.File(fileName,'r')\n",
        "            weight = []\n",
        "            for i in range(len(fid.keys())):\n",
        "                weight.append(fid['weight'+str(i)][:])\n",
        "            model.set_weights(weight)\n",
        "\n",
        "        def schedule(epoch):\n",
        "            lr = K.get_value(model.optimizer.lr)\n",
        "            if epoch % self._param['step_inter'] == 0 and epoch > 0:\n",
        "                lr = lr*self._param['lr_gamma']\n",
        "            return np.float(lr)\n",
        "\n",
        "        write_file = False\n",
        "        if self._param['write_file']:\n",
        "            write_file = True\n",
        "            fid_out = open(self._param['write_file_name'], 'w') #deep_bkp.txt\n",
        "\n",
        "        save_model = False\n",
        "        if self._param['save_model']:\n",
        "            save_model = True\n",
        "            save_path = self._param['save_path'] #'data/save_param_temp/deep_bkp'\n",
        "\n",
        "        if self._param['initial_file'] != None:\n",
        "            read_hdf5(model, self._param['initial_file'] )\n",
        "\n",
        "        class evaluateVal(keras.callbacks.Callback):\n",
        "            def __init__(self, vid_list):\n",
        "                self.group_list, self.gt_val = self.merge_list(vid_list)\n",
        "\n",
        "            def merge_list(self, vid_list):\n",
        "                group_list = []\n",
        "                gt_val = []\n",
        "                idx_per = []\n",
        "                for idx, name in enumerate(vid_list):\n",
        "                    if idx == len(vid_list)-1:\n",
        "                        last_name = ''\n",
        "                    else:\n",
        "                        last_name = vid_list[idx+1]\n",
        "                    if name != last_name:\n",
        "                        idx_per.append(idx)\n",
        "                        gt_val.append(np.argmax(valY[idx]) )\n",
        "                        group_list.append(np.asarray(idx_per) )\n",
        "                        idx_per = []\n",
        "                    else:\n",
        "                        idx_per.append(idx)\n",
        "                return group_list, gt_val\n",
        "\n",
        "            def on_epoch_end(self, epoch, logs={}):\n",
        "               \n",
        "                #if ((epoch) % 2==0):\n",
        "                if 1:\n",
        "                    # val_loss = model.evaluate(valX, valY, batch_size=512, verbose=0)[0]\n",
        "                    prob_val = model.predict(valX, batch_size=512, verbose=0)\n",
        "                    pred = np.asarray([np.argmax(np.mean(prob_val[idx], axis=0)) for idx in self.group_list ] )\n",
        "                    acc = sum( int(pred[i]) == self.gt_val[i] for i in range(len(self.gt_val))) / float(len(self.gt_val))\n",
        "                    #train_loss = model.evaluate(trainX, trainY, batch_size=512, verbose=0)[0]\n",
        "                    #cmd_str1 = 'evluation epoch=%d, learn_rate=%f, train loss=%f, validation loss=%f, validation accuracy=%f' % (epoch,\n",
        "                    #K.get_value(model.optimizer.lr), train_loss, val_loss, acc)\n",
        "                    cmd_str = 'evluation epoch=%d, learn_rate=%f, validation accuracy=%f' % (epoch+1, K.get_value(model.optimizer.lr), acc)\n",
        "                    print( cmd_str)\n",
        "           \n",
        "                    #print( cmd_str1)\n",
        "                    # if 'fid_out' in locals() or 'fid_out' in globals():\n",
        "                    if write_file:\n",
        "                        fid_out.write(cmd_str + '\\n')\n",
        "                    if (epoch % 4==0) and save_model:\n",
        "#                     if acc>0.85 and save_model:\n",
        "                        save_file = save_path + ('_epoch%d.h5' % (epoch+1)) #'data/save_param_temp/deep_bkp'\n",
        "                        if os.path.exists(save_file):\n",
        "                            os.remove(save_file)\n",
        "                        # model.save_weights(save_file)\n",
        "                        save_hdf5(model, save_file)\n",
        "        \n",
        "\n",
        "        reduce_lr = LearningRateScheduler(schedule)\n",
        "        # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=0.001)\n",
        "\n",
        "        evaluate_val = evaluateVal(val_vid_list)\n",
        "        model.fit(trainX, trainY, batch_size=self._param['batchsize'], epochs=self._param['max_iter'],\n",
        "                  callbacks=[evaluate_val, reduce_lr ], shuffle=True, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "def run_model():\n",
        "    param = {}\n",
        "    param['max_iter'] = 200\n",
        "    param['step_inter'] = 40\n",
        "    param['base_learn_rate'] = 0.01 #  defaults 0.02\n",
        "    # param['base_learn_rate'] = 0.001250 # finetune learning rate\n",
        "    param['lr_gamma'] = 0.5\n",
        "    param['weight_regular'] = 0\n",
        "    param['batchsize'] = 128 # previous 64\n",
        "    # for multi-scale model, 512 output of memory\n",
        "    param['num_seq'] = 100\n",
        "\n",
        "    if 1:\n",
        "        param['trn_arr_file'] = 'drive/Colab Notebooks/thesis/view_seq_1_10/new_array_list_train.h5'\n",
        "        param['trn_lst_file'] = 'drive/Colab Notebooks/thesis/view_seq_1_10/new_file_list_train.txt'\n",
        "        param['tst_arr_file'] = 'drive/Colab Notebooks/thesis/view_seq_1_10/new_array_list_test.h5'\n",
        "        param['tst_lst_file'] = 'drive/Colab Notebooks/thesis/view_seq_1_10/new_file_list_test.txt'\n",
        "    else:\n",
        "        param['trn_arr_file'] = 'data/subj_seq/new_array_list_train.h5'\n",
        "        param['trn_lst_file'] = 'data/subj_seq/new_file_list_train.txt'\n",
        "        param['tst_arr_file'] = 'data/subj_seq/new_array_list_test.h5'\n",
        "        # param['trn_angle_file'] = '../data/subj_seq/new_angle_list_train.txt'\n",
        "        param['tst_lst_file'] = 'data/subj_seq/new_file_list_test.txt'\n",
        "        # param['tst_angle_file'] = '../data/subj_seq/new_angle_list_test.txt'\n",
        "\n",
        "    param['write_file'] = True\n",
        "    param['write_file_name'] = '分类1层数3速度0.01降比10关节85.txt' # 'subj.txt', 'view.txt'\n",
        "    param['save_model'] = True\n",
        "    param['save_path'] = 'drive/Colab Notebooks/thesis/分类1层数3速度0.01降比10关节85'\n",
        "    param['initial_file'] = None\n",
        "\n",
        "    model = construct_model(param)\n",
        "    # model.group_person_list('data/view_seq/new_file_list_train.txt')\n",
        "    model.train_model()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    run_model()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100, 18, 2)        0         \n",
            "_________________________________________________________________\n",
            "transform_layer_1 (Transform (None, 100, 18, 2)        0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 100, 36)           0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 100, 36)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 1024)         2248704   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_2 (Spatial (None, 100, 1024)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 100, 1024)         6295552   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_3 (Spatial (None, 100, 1024)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 100, 1024)         6295552   \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 14,841,858\n",
            "Trainable params: 14,841,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "load fall number 106\n",
            "load fall number from file 106\n",
            "load total number 528\n",
            "load fall number 158\n",
            "load fall number from file 158\n",
            "load total number 796\n",
            "train data: (796, 100, 18, 2) (796, 2)\n",
            "test data: (528, 100, 18, 2) (528, 2)\n",
            "train fall data 638\n",
            "test fall data 106\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/200\n",
            "796/796 [==============================] - 13s 16ms/step - loss: 0.6220 - acc: 0.7312\n",
            "evluation epoch=1, learn_rate=0.010000, validation accuracy=0.799242\n",
            "Epoch 2/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.5465 - acc: 0.8003\n",
            "evluation epoch=2, learn_rate=0.010000, validation accuracy=0.799242\n",
            "Epoch 3/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.4910 - acc: 0.7965\n",
            "evluation epoch=3, learn_rate=0.010000, validation accuracy=0.791667\n",
            "Epoch 4/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.4698 - acc: 0.7952\n",
            "evluation epoch=4, learn_rate=0.010000, validation accuracy=0.757576\n",
            "Epoch 5/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.4359 - acc: 0.8178\n",
            "evluation epoch=5, learn_rate=0.010000, validation accuracy=0.761364\n",
            "Epoch 6/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.4130 - acc: 0.8229\n",
            "evluation epoch=6, learn_rate=0.010000, validation accuracy=0.816288\n",
            "Epoch 7/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.3812 - acc: 0.8304\n",
            "evluation epoch=7, learn_rate=0.010000, validation accuracy=0.818182\n",
            "Epoch 8/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.3624 - acc: 0.8317\n",
            "evluation epoch=8, learn_rate=0.010000, validation accuracy=0.804924\n",
            "Epoch 9/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.3276 - acc: 0.8668\n",
            "evluation epoch=9, learn_rate=0.010000, validation accuracy=0.821970\n",
            "Epoch 10/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.3267 - acc: 0.8518\n",
            "evluation epoch=10, learn_rate=0.010000, validation accuracy=0.734848\n",
            "Epoch 11/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.3018 - acc: 0.8731\n",
            "evluation epoch=11, learn_rate=0.010000, validation accuracy=0.799242\n",
            "Epoch 12/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.2527 - acc: 0.9083\n",
            "evluation epoch=12, learn_rate=0.010000, validation accuracy=0.795455\n",
            "Epoch 13/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.2153 - acc: 0.9309\n",
            "evluation epoch=13, learn_rate=0.010000, validation accuracy=0.801136\n",
            "Epoch 14/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.2131 - acc: 0.9271\n",
            "evluation epoch=14, learn_rate=0.010000, validation accuracy=0.740530\n",
            "Epoch 15/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.1866 - acc: 0.9397\n",
            "evluation epoch=15, learn_rate=0.010000, validation accuracy=0.823864\n",
            "Epoch 16/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1834 - acc: 0.9234\n",
            "evluation epoch=16, learn_rate=0.010000, validation accuracy=0.770833\n",
            "Epoch 17/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.1398 - acc: 0.9611\n",
            "evluation epoch=17, learn_rate=0.010000, validation accuracy=0.823864\n",
            "Epoch 18/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1351 - acc: 0.9548\n",
            "evluation epoch=18, learn_rate=0.010000, validation accuracy=0.727273\n",
            "Epoch 19/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1458 - acc: 0.9447\n",
            "evluation epoch=19, learn_rate=0.010000, validation accuracy=0.823864\n",
            "Epoch 20/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1631 - acc: 0.9422\n",
            "evluation epoch=20, learn_rate=0.010000, validation accuracy=0.823864\n",
            "Epoch 21/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1290 - acc: 0.9560\n",
            "evluation epoch=21, learn_rate=0.010000, validation accuracy=0.814394\n",
            "Epoch 22/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1460 - acc: 0.9447\n",
            "evluation epoch=22, learn_rate=0.010000, validation accuracy=0.825758\n",
            "Epoch 23/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.1564 - acc: 0.9359\n",
            "evluation epoch=23, learn_rate=0.010000, validation accuracy=0.818182\n",
            "Epoch 24/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.1600 - acc: 0.9296\n",
            "evluation epoch=24, learn_rate=0.010000, validation accuracy=0.839015\n",
            "Epoch 25/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1211 - acc: 0.9636\n",
            "evluation epoch=25, learn_rate=0.010000, validation accuracy=0.795455\n",
            "Epoch 26/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.1390 - acc: 0.9460\n",
            "evluation epoch=26, learn_rate=0.010000, validation accuracy=0.797348\n",
            "Epoch 27/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.2094 - acc: 0.9146\n",
            "evluation epoch=27, learn_rate=0.010000, validation accuracy=0.761364\n",
            "Epoch 28/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1635 - acc: 0.9372\n",
            "evluation epoch=28, learn_rate=0.010000, validation accuracy=0.708333\n",
            "Epoch 29/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1379 - acc: 0.9523\n",
            "evluation epoch=29, learn_rate=0.010000, validation accuracy=0.725379\n",
            "Epoch 30/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1154 - acc: 0.9573\n",
            "evluation epoch=30, learn_rate=0.010000, validation accuracy=0.659091\n",
            "Epoch 31/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1543 - acc: 0.9359\n",
            "evluation epoch=31, learn_rate=0.010000, validation accuracy=0.799242\n",
            "Epoch 32/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.1348 - acc: 0.9548\n",
            "evluation epoch=32, learn_rate=0.010000, validation accuracy=0.763258\n",
            "Epoch 33/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.1072 - acc: 0.9648\n",
            "evluation epoch=33, learn_rate=0.010000, validation accuracy=0.812500\n",
            "Epoch 34/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1100 - acc: 0.9573\n",
            "evluation epoch=34, learn_rate=0.010000, validation accuracy=0.795455\n",
            "Epoch 35/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1020 - acc: 0.9585\n",
            "evluation epoch=35, learn_rate=0.010000, validation accuracy=0.757576\n",
            "Epoch 36/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1025 - acc: 0.9711\n",
            "evluation epoch=36, learn_rate=0.010000, validation accuracy=0.850379\n",
            "Epoch 37/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0923 - acc: 0.9686\n",
            "evluation epoch=37, learn_rate=0.010000, validation accuracy=0.806818\n",
            "Epoch 38/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1032 - acc: 0.9598\n",
            "evluation epoch=38, learn_rate=0.010000, validation accuracy=0.839015\n",
            "Epoch 39/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0999 - acc: 0.9648\n",
            "evluation epoch=39, learn_rate=0.010000, validation accuracy=0.723485\n",
            "Epoch 40/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.1159 - acc: 0.9611\n",
            "evluation epoch=40, learn_rate=0.010000, validation accuracy=0.784091\n",
            "Epoch 41/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0849 - acc: 0.9711\n",
            "evluation epoch=41, learn_rate=0.005000, validation accuracy=0.842803\n",
            "Epoch 42/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.1285 - acc: 0.9560\n",
            "evluation epoch=42, learn_rate=0.005000, validation accuracy=0.715909\n",
            "Epoch 43/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.1165 - acc: 0.9585\n",
            "evluation epoch=43, learn_rate=0.005000, validation accuracy=0.785985\n",
            "Epoch 44/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0855 - acc: 0.9724\n",
            "evluation epoch=44, learn_rate=0.005000, validation accuracy=0.801136\n",
            "Epoch 45/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0747 - acc: 0.9774\n",
            "evluation epoch=45, learn_rate=0.005000, validation accuracy=0.840909\n",
            "Epoch 46/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0598 - acc: 0.9824\n",
            "evluation epoch=46, learn_rate=0.005000, validation accuracy=0.757576\n",
            "Epoch 47/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0875 - acc: 0.9648\n",
            "evluation epoch=47, learn_rate=0.005000, validation accuracy=0.748106\n",
            "Epoch 48/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0668 - acc: 0.9799\n",
            "evluation epoch=48, learn_rate=0.005000, validation accuracy=0.751894\n",
            "Epoch 49/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0513 - acc: 0.9887\n",
            "evluation epoch=49, learn_rate=0.005000, validation accuracy=0.778409\n",
            "Epoch 50/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0602 - acc: 0.9761\n",
            "evluation epoch=50, learn_rate=0.005000, validation accuracy=0.804924\n",
            "Epoch 51/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0661 - acc: 0.9774\n",
            "evluation epoch=51, learn_rate=0.005000, validation accuracy=0.803030\n",
            "Epoch 52/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0596 - acc: 0.9812\n",
            "evluation epoch=52, learn_rate=0.005000, validation accuracy=0.797348\n",
            "Epoch 53/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0573 - acc: 0.9849\n",
            "evluation epoch=53, learn_rate=0.005000, validation accuracy=0.785985\n",
            "Epoch 54/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0683 - acc: 0.9786\n",
            "evluation epoch=54, learn_rate=0.005000, validation accuracy=0.803030\n",
            "Epoch 55/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0550 - acc: 0.9812\n",
            "evluation epoch=55, learn_rate=0.005000, validation accuracy=0.784091\n",
            "Epoch 56/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0484 - acc: 0.9837\n",
            "evluation epoch=56, learn_rate=0.005000, validation accuracy=0.808712\n",
            "Epoch 57/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0463 - acc: 0.9812\n",
            "evluation epoch=57, learn_rate=0.005000, validation accuracy=0.803030\n",
            "Epoch 58/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0455 - acc: 0.9824\n",
            "evluation epoch=58, learn_rate=0.005000, validation accuracy=0.731061\n",
            "Epoch 59/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0322 - acc: 0.9899\n",
            "evluation epoch=59, learn_rate=0.005000, validation accuracy=0.787879\n",
            "Epoch 60/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0460 - acc: 0.9887\n",
            "evluation epoch=60, learn_rate=0.005000, validation accuracy=0.770833\n",
            "Epoch 61/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0433 - acc: 0.9874\n",
            "evluation epoch=61, learn_rate=0.005000, validation accuracy=0.785985\n",
            "Epoch 62/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0427 - acc: 0.9887\n",
            "evluation epoch=62, learn_rate=0.005000, validation accuracy=0.759470\n",
            "Epoch 63/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0482 - acc: 0.9874\n",
            "evluation epoch=63, learn_rate=0.005000, validation accuracy=0.774621\n",
            "Epoch 64/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0540 - acc: 0.9786\n",
            "evluation epoch=64, learn_rate=0.005000, validation accuracy=0.751894\n",
            "Epoch 65/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0520 - acc: 0.9837\n",
            "evluation epoch=65, learn_rate=0.005000, validation accuracy=0.814394\n",
            "Epoch 66/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0373 - acc: 0.9937\n",
            "evluation epoch=66, learn_rate=0.005000, validation accuracy=0.797348\n",
            "Epoch 67/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0342 - acc: 0.9937\n",
            "evluation epoch=67, learn_rate=0.005000, validation accuracy=0.797348\n",
            "Epoch 68/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0315 - acc: 0.9950\n",
            "evluation epoch=68, learn_rate=0.005000, validation accuracy=0.825758\n",
            "Epoch 69/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0268 - acc: 0.9937\n",
            "evluation epoch=69, learn_rate=0.005000, validation accuracy=0.757576\n",
            "Epoch 70/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0399 - acc: 0.9862\n",
            "evluation epoch=70, learn_rate=0.005000, validation accuracy=0.827652\n",
            "Epoch 71/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0375 - acc: 0.9899\n",
            "evluation epoch=71, learn_rate=0.005000, validation accuracy=0.808712\n",
            "Epoch 72/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0436 - acc: 0.9824\n",
            "evluation epoch=72, learn_rate=0.005000, validation accuracy=0.825758\n",
            "Epoch 73/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0376 - acc: 0.9912\n",
            "evluation epoch=73, learn_rate=0.005000, validation accuracy=0.863636\n",
            "Epoch 74/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0540 - acc: 0.9849\n",
            "evluation epoch=74, learn_rate=0.005000, validation accuracy=0.791667\n",
            "Epoch 75/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0560 - acc: 0.9874\n",
            "evluation epoch=75, learn_rate=0.005000, validation accuracy=0.850379\n",
            "Epoch 76/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0288 - acc: 0.9912\n",
            "evluation epoch=76, learn_rate=0.005000, validation accuracy=0.761364\n",
            "Epoch 77/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0531 - acc: 0.9824\n",
            "evluation epoch=77, learn_rate=0.005000, validation accuracy=0.878788\n",
            "Epoch 78/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0647 - acc: 0.9786\n",
            "evluation epoch=78, learn_rate=0.005000, validation accuracy=0.842803\n",
            "Epoch 79/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0402 - acc: 0.9887\n",
            "evluation epoch=79, learn_rate=0.005000, validation accuracy=0.867424\n",
            "Epoch 80/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0297 - acc: 0.9925\n",
            "evluation epoch=80, learn_rate=0.005000, validation accuracy=0.835227\n",
            "Epoch 81/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0375 - acc: 0.9887\n",
            "evluation epoch=81, learn_rate=0.002500, validation accuracy=0.789773\n",
            "Epoch 82/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0355 - acc: 0.9899\n",
            "evluation epoch=82, learn_rate=0.002500, validation accuracy=0.816288\n",
            "Epoch 83/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0347 - acc: 0.9874\n",
            "evluation epoch=83, learn_rate=0.002500, validation accuracy=0.784091\n",
            "Epoch 84/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0262 - acc: 0.9899\n",
            "evluation epoch=84, learn_rate=0.002500, validation accuracy=0.812500\n",
            "Epoch 85/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0228 - acc: 0.9950\n",
            "evluation epoch=85, learn_rate=0.002500, validation accuracy=0.825758\n",
            "Epoch 86/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0221 - acc: 0.9950\n",
            "evluation epoch=86, learn_rate=0.002500, validation accuracy=0.818182\n",
            "Epoch 87/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0283 - acc: 0.9925\n",
            "evluation epoch=87, learn_rate=0.002500, validation accuracy=0.810606\n",
            "Epoch 88/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0279 - acc: 0.9925\n",
            "evluation epoch=88, learn_rate=0.002500, validation accuracy=0.818182\n",
            "Epoch 89/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0286 - acc: 0.9912\n",
            "evluation epoch=89, learn_rate=0.002500, validation accuracy=0.795455\n",
            "Epoch 90/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0426 - acc: 0.9862\n",
            "evluation epoch=90, learn_rate=0.002500, validation accuracy=0.812500\n",
            "Epoch 91/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0338 - acc: 0.9899\n",
            "evluation epoch=91, learn_rate=0.002500, validation accuracy=0.833333\n",
            "Epoch 92/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0400 - acc: 0.9862\n",
            "evluation epoch=92, learn_rate=0.002500, validation accuracy=0.812500\n",
            "Epoch 93/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0203 - acc: 0.9937\n",
            "evluation epoch=93, learn_rate=0.002500, validation accuracy=0.820076\n",
            "Epoch 94/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0393 - acc: 0.9862\n",
            "evluation epoch=94, learn_rate=0.002500, validation accuracy=0.833333\n",
            "Epoch 95/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0210 - acc: 0.9950\n",
            "evluation epoch=95, learn_rate=0.002500, validation accuracy=0.814394\n",
            "Epoch 96/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0252 - acc: 0.9925\n",
            "evluation epoch=96, learn_rate=0.002500, validation accuracy=0.812500\n",
            "Epoch 97/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0208 - acc: 0.9937\n",
            "evluation epoch=97, learn_rate=0.002500, validation accuracy=0.823864\n",
            "Epoch 98/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0234 - acc: 0.9925\n",
            "evluation epoch=98, learn_rate=0.002500, validation accuracy=0.803030\n",
            "Epoch 99/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0306 - acc: 0.9874\n",
            "evluation epoch=99, learn_rate=0.002500, validation accuracy=0.831439\n",
            "Epoch 100/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0317 - acc: 0.9887\n",
            "evluation epoch=100, learn_rate=0.002500, validation accuracy=0.818182\n",
            "Epoch 101/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0212 - acc: 0.9925\n",
            "evluation epoch=101, learn_rate=0.002500, validation accuracy=0.801136\n",
            "Epoch 102/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0273 - acc: 0.9912\n",
            "evluation epoch=102, learn_rate=0.002500, validation accuracy=0.812500\n",
            "Epoch 103/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0254 - acc: 0.9925\n",
            "evluation epoch=103, learn_rate=0.002500, validation accuracy=0.829545\n",
            "Epoch 104/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0320 - acc: 0.9912\n",
            "evluation epoch=104, learn_rate=0.002500, validation accuracy=0.806818\n",
            "Epoch 105/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0197 - acc: 0.9937\n",
            "evluation epoch=105, learn_rate=0.002500, validation accuracy=0.746212\n",
            "Epoch 106/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0413 - acc: 0.9899\n",
            "evluation epoch=106, learn_rate=0.002500, validation accuracy=0.810606\n",
            "Epoch 107/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0186 - acc: 0.9962\n",
            "evluation epoch=107, learn_rate=0.002500, validation accuracy=0.829545\n",
            "Epoch 108/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0226 - acc: 0.9925\n",
            "evluation epoch=108, learn_rate=0.002500, validation accuracy=0.803030\n",
            "Epoch 109/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0261 - acc: 0.9912\n",
            "evluation epoch=109, learn_rate=0.002500, validation accuracy=0.825758\n",
            "Epoch 110/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0212 - acc: 0.9925\n",
            "evluation epoch=110, learn_rate=0.002500, validation accuracy=0.837121\n",
            "Epoch 111/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0243 - acc: 0.9899\n",
            "evluation epoch=111, learn_rate=0.002500, validation accuracy=0.840909\n",
            "Epoch 112/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0389 - acc: 0.9887\n",
            "evluation epoch=112, learn_rate=0.002500, validation accuracy=0.823864\n",
            "Epoch 113/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0298 - acc: 0.9912\n",
            "evluation epoch=113, learn_rate=0.002500, validation accuracy=0.823864\n",
            "Epoch 114/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0235 - acc: 0.9937\n",
            "evluation epoch=114, learn_rate=0.002500, validation accuracy=0.833333\n",
            "Epoch 115/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0216 - acc: 0.9962\n",
            "evluation epoch=115, learn_rate=0.002500, validation accuracy=0.823864\n",
            "Epoch 116/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0256 - acc: 0.9912\n",
            "evluation epoch=116, learn_rate=0.002500, validation accuracy=0.840909\n",
            "Epoch 117/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0209 - acc: 0.9950\n",
            "evluation epoch=117, learn_rate=0.002500, validation accuracy=0.837121\n",
            "Epoch 118/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0227 - acc: 0.9899\n",
            "evluation epoch=118, learn_rate=0.002500, validation accuracy=0.823864\n",
            "Epoch 119/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0330 - acc: 0.9912\n",
            "evluation epoch=119, learn_rate=0.002500, validation accuracy=0.827652\n",
            "Epoch 120/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0197 - acc: 0.9937\n",
            "evluation epoch=120, learn_rate=0.002500, validation accuracy=0.801136\n",
            "Epoch 121/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0253 - acc: 0.9925\n",
            "evluation epoch=121, learn_rate=0.001250, validation accuracy=0.799242\n",
            "Epoch 122/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0210 - acc: 0.9937\n",
            "evluation epoch=122, learn_rate=0.001250, validation accuracy=0.844697\n",
            "Epoch 123/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0389 - acc: 0.9862\n",
            "evluation epoch=123, learn_rate=0.001250, validation accuracy=0.840909\n",
            "Epoch 124/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0173 - acc: 0.9950\n",
            "evluation epoch=124, learn_rate=0.001250, validation accuracy=0.823864\n",
            "Epoch 125/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0178 - acc: 0.9925\n",
            "evluation epoch=125, learn_rate=0.001250, validation accuracy=0.806818\n",
            "Epoch 126/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0185 - acc: 0.9937\n",
            "evluation epoch=126, learn_rate=0.001250, validation accuracy=0.789773\n",
            "Epoch 127/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0141 - acc: 0.9975\n",
            "evluation epoch=127, learn_rate=0.001250, validation accuracy=0.801136\n",
            "Epoch 128/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0170 - acc: 0.9937\n",
            "evluation epoch=128, learn_rate=0.001250, validation accuracy=0.818182\n",
            "Epoch 129/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0172 - acc: 0.9962\n",
            "evluation epoch=129, learn_rate=0.001250, validation accuracy=0.818182\n",
            "Epoch 130/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0151 - acc: 0.9950\n",
            "evluation epoch=130, learn_rate=0.001250, validation accuracy=0.823864\n",
            "Epoch 131/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0145 - acc: 0.9950\n",
            "evluation epoch=131, learn_rate=0.001250, validation accuracy=0.827652\n",
            "Epoch 132/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0197 - acc: 0.9937\n",
            "evluation epoch=132, learn_rate=0.001250, validation accuracy=0.814394\n",
            "Epoch 133/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0231 - acc: 0.9937\n",
            "evluation epoch=133, learn_rate=0.001250, validation accuracy=0.804924\n",
            "Epoch 134/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0176 - acc: 0.9962\n",
            "evluation epoch=134, learn_rate=0.001250, validation accuracy=0.818182\n",
            "Epoch 135/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0174 - acc: 0.9962\n",
            "evluation epoch=135, learn_rate=0.001250, validation accuracy=0.814394\n",
            "Epoch 136/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0245 - acc: 0.9925\n",
            "evluation epoch=136, learn_rate=0.001250, validation accuracy=0.804924\n",
            "Epoch 137/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0179 - acc: 0.9950\n",
            "evluation epoch=137, learn_rate=0.001250, validation accuracy=0.804924\n",
            "Epoch 138/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0150 - acc: 0.9962\n",
            "evluation epoch=138, learn_rate=0.001250, validation accuracy=0.816288\n",
            "Epoch 139/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0206 - acc: 0.9912\n",
            "evluation epoch=139, learn_rate=0.001250, validation accuracy=0.825758\n",
            "Epoch 140/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0178 - acc: 0.9925\n",
            "evluation epoch=140, learn_rate=0.001250, validation accuracy=0.827652\n",
            "Epoch 141/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0223 - acc: 0.9950\n",
            "evluation epoch=141, learn_rate=0.001250, validation accuracy=0.814394\n",
            "Epoch 142/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0163 - acc: 0.9950\n",
            "evluation epoch=142, learn_rate=0.001250, validation accuracy=0.825758\n",
            "Epoch 143/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0174 - acc: 0.9962\n",
            "evluation epoch=143, learn_rate=0.001250, validation accuracy=0.829545\n",
            "Epoch 144/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0242 - acc: 0.9899\n",
            "evluation epoch=144, learn_rate=0.001250, validation accuracy=0.821970\n",
            "Epoch 145/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0131 - acc: 0.9975\n",
            "evluation epoch=145, learn_rate=0.001250, validation accuracy=0.814394\n",
            "Epoch 146/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0143 - acc: 0.9962\n",
            "evluation epoch=146, learn_rate=0.001250, validation accuracy=0.823864\n",
            "Epoch 147/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0156 - acc: 0.9937\n",
            "evluation epoch=147, learn_rate=0.001250, validation accuracy=0.821970\n",
            "Epoch 148/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0198 - acc: 0.9925\n",
            "evluation epoch=148, learn_rate=0.001250, validation accuracy=0.827652\n",
            "Epoch 149/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0115 - acc: 0.9987\n",
            "evluation epoch=149, learn_rate=0.001250, validation accuracy=0.844697\n",
            "Epoch 150/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0226 - acc: 0.9912\n",
            "evluation epoch=150, learn_rate=0.001250, validation accuracy=0.848485\n",
            "Epoch 151/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0153 - acc: 0.9950\n",
            "evluation epoch=151, learn_rate=0.001250, validation accuracy=0.833333\n",
            "Epoch 152/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0143 - acc: 0.9962\n",
            "evluation epoch=152, learn_rate=0.001250, validation accuracy=0.844697\n",
            "Epoch 153/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0217 - acc: 0.9937\n",
            "evluation epoch=153, learn_rate=0.001250, validation accuracy=0.848485\n",
            "Epoch 154/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0149 - acc: 0.9950\n",
            "evluation epoch=154, learn_rate=0.001250, validation accuracy=0.842803\n",
            "Epoch 155/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0158 - acc: 0.9937\n",
            "evluation epoch=155, learn_rate=0.001250, validation accuracy=0.833333\n",
            "Epoch 156/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0127 - acc: 0.9962\n",
            "evluation epoch=156, learn_rate=0.001250, validation accuracy=0.829545\n",
            "Epoch 157/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0083 - acc: 0.9975\n",
            "evluation epoch=157, learn_rate=0.001250, validation accuracy=0.820076\n",
            "Epoch 158/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0223 - acc: 0.9950\n",
            "evluation epoch=158, learn_rate=0.001250, validation accuracy=0.806818\n",
            "Epoch 159/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0102 - acc: 0.9987\n",
            "evluation epoch=159, learn_rate=0.001250, validation accuracy=0.808712\n",
            "Epoch 160/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0163 - acc: 0.9937\n",
            "evluation epoch=160, learn_rate=0.001250, validation accuracy=0.835227\n",
            "Epoch 161/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0191 - acc: 0.9962\n",
            "evluation epoch=161, learn_rate=0.000625, validation accuracy=0.842803\n",
            "Epoch 162/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0184 - acc: 0.9925\n",
            "evluation epoch=162, learn_rate=0.000625, validation accuracy=0.829545\n",
            "Epoch 163/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0138 - acc: 0.9950\n",
            "evluation epoch=163, learn_rate=0.000625, validation accuracy=0.827652\n",
            "Epoch 164/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0170 - acc: 0.9937\n",
            "evluation epoch=164, learn_rate=0.000625, validation accuracy=0.820076\n",
            "Epoch 165/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0236 - acc: 0.9937\n",
            "evluation epoch=165, learn_rate=0.000625, validation accuracy=0.823864\n",
            "Epoch 166/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0129 - acc: 0.9962\n",
            "evluation epoch=166, learn_rate=0.000625, validation accuracy=0.823864\n",
            "Epoch 167/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0149 - acc: 0.9950\n",
            "evluation epoch=167, learn_rate=0.000625, validation accuracy=0.825758\n",
            "Epoch 168/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0083 - acc: 0.9987\n",
            "evluation epoch=168, learn_rate=0.000625, validation accuracy=0.829545\n",
            "Epoch 169/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0212 - acc: 0.9950\n",
            "evluation epoch=169, learn_rate=0.000625, validation accuracy=0.823864\n",
            "Epoch 170/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0122 - acc: 0.9975\n",
            "evluation epoch=170, learn_rate=0.000625, validation accuracy=0.827652\n",
            "Epoch 171/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0094 - acc: 0.9975\n",
            "evluation epoch=171, learn_rate=0.000625, validation accuracy=0.829545\n",
            "Epoch 172/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0091 - acc: 0.9975\n",
            "evluation epoch=172, learn_rate=0.000625, validation accuracy=0.829545\n",
            "Epoch 173/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0201 - acc: 0.9937\n",
            "evluation epoch=173, learn_rate=0.000625, validation accuracy=0.820076\n",
            "Epoch 174/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0152 - acc: 0.9950\n",
            "evluation epoch=174, learn_rate=0.000625, validation accuracy=0.821970\n",
            "Epoch 175/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0285 - acc: 0.9912\n",
            "evluation epoch=175, learn_rate=0.000625, validation accuracy=0.820076\n",
            "Epoch 176/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0158 - acc: 0.9937\n",
            "evluation epoch=176, learn_rate=0.000625, validation accuracy=0.816288\n",
            "Epoch 177/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0104 - acc: 0.9962\n",
            "evluation epoch=177, learn_rate=0.000625, validation accuracy=0.820076\n",
            "Epoch 178/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0092 - acc: 0.9975\n",
            "evluation epoch=178, learn_rate=0.000625, validation accuracy=0.818182\n",
            "Epoch 179/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0130 - acc: 0.9962\n",
            "evluation epoch=179, learn_rate=0.000625, validation accuracy=0.808712\n",
            "Epoch 180/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0115 - acc: 0.9962\n",
            "evluation epoch=180, learn_rate=0.000625, validation accuracy=0.803030\n",
            "Epoch 181/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0185 - acc: 0.9937\n",
            "evluation epoch=181, learn_rate=0.000625, validation accuracy=0.806818\n",
            "Epoch 182/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0210 - acc: 0.9925\n",
            "evluation epoch=182, learn_rate=0.000625, validation accuracy=0.810606\n",
            "Epoch 183/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0126 - acc: 0.9975\n",
            "evluation epoch=183, learn_rate=0.000625, validation accuracy=0.820076\n",
            "Epoch 184/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0102 - acc: 0.9975\n",
            "evluation epoch=184, learn_rate=0.000625, validation accuracy=0.821970\n",
            "Epoch 185/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0083 - acc: 0.9975\n",
            "evluation epoch=185, learn_rate=0.000625, validation accuracy=0.825758\n",
            "Epoch 186/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0112 - acc: 0.9975\n",
            "evluation epoch=186, learn_rate=0.000625, validation accuracy=0.825758\n",
            "Epoch 187/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0087 - acc: 0.9975\n",
            "evluation epoch=187, learn_rate=0.000625, validation accuracy=0.823864\n",
            "Epoch 188/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0126 - acc: 0.9962\n",
            "evluation epoch=188, learn_rate=0.000625, validation accuracy=0.823864\n",
            "Epoch 189/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0151 - acc: 0.9962\n",
            "evluation epoch=189, learn_rate=0.000625, validation accuracy=0.823864\n",
            "Epoch 190/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0215 - acc: 0.9937\n",
            "evluation epoch=190, learn_rate=0.000625, validation accuracy=0.816288\n",
            "Epoch 191/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0143 - acc: 0.9937\n",
            "evluation epoch=191, learn_rate=0.000625, validation accuracy=0.816288\n",
            "Epoch 192/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0204 - acc: 0.9950\n",
            "evluation epoch=192, learn_rate=0.000625, validation accuracy=0.821970\n",
            "Epoch 193/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0147 - acc: 0.9950\n",
            "evluation epoch=193, learn_rate=0.000625, validation accuracy=0.812500\n",
            "Epoch 194/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0127 - acc: 0.9962\n",
            "evluation epoch=194, learn_rate=0.000625, validation accuracy=0.812500\n",
            "Epoch 195/200\n",
            "796/796 [==============================] - 7s 9ms/step - loss: 0.0121 - acc: 0.9950\n",
            "evluation epoch=195, learn_rate=0.000625, validation accuracy=0.818182\n",
            "Epoch 196/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0223 - acc: 0.9950\n",
            "evluation epoch=196, learn_rate=0.000625, validation accuracy=0.821970\n",
            "Epoch 197/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0165 - acc: 0.9937\n",
            "evluation epoch=197, learn_rate=0.000625, validation accuracy=0.823864\n",
            "Epoch 198/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0107 - acc: 0.9962\n",
            "evluation epoch=198, learn_rate=0.000625, validation accuracy=0.820076\n",
            "Epoch 199/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0108 - acc: 0.9987\n",
            "evluation epoch=199, learn_rate=0.000625, validation accuracy=0.820076\n",
            "Epoch 200/200\n",
            "796/796 [==============================] - 7s 8ms/step - loss: 0.0079 - acc: 1.0000\n",
            "evluation epoch=200, learn_rate=0.000625, validation accuracy=0.829545\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}