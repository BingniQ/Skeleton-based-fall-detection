{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BingniQ/Skeleton-based-fall-detection/blob/master/deep_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpguCr0z6HNP",
        "colab_type": "code",
        "outputId": "7345213a-0e13-497c-e8c5-b6bef37c8e93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import scipy\n",
        "from scipy import linalg\n",
        "import random\n",
        "import math\n",
        "import h5py\n",
        "import theano\n",
        "import keras\n",
        "from theano import tensor as T\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, Bidirectional, \\\n",
        "    GRU, SimpleRNN, Input, SpatialDropout1D, Reshape, Permute, merge, Lambda\n",
        "from keras.layers.merge import Add, Concatenate, Maximum\n",
        "from keras.layers.convolutional import Convolution2D,Convolution3D\n",
        "from keras.layers.pooling import MaxPooling1D\n",
        "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.optimizers import RMSprop,SGD,Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras.regularizers import l2, l1\n",
        "from keras.constraints import maxnorm, unitnorm\n",
        "import tensorflow as tf\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkfE8BHckY9y",
        "colab_type": "code",
        "outputId": "3fb5c7a4-23e0-4ab1-a675-7428c92e5223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130811 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6wNifbgekKX",
        "colab_type": "code",
        "outputId": "ea6ae78d-0224-4f96-e8f0-793731126f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "from google.colab import drive\n",
        "!mkdir drive\n",
        "!google-drive-ocamlfuse drive\n",
        "!ls drive/\"Colab Notebooks/data/subj_seq_1_20\"\n",
        "#f = open(\"drive/Colab Notebooks/data/subj_seq/new_file_list_test.txt\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘drive’: File exists\n",
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n",
            "new_array_list_test.h5\t new_file_list_test.txt\n",
            "new_array_list_train.h5  new_file_list_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVWtSVv66hlv",
        "colab_type": "code",
        "outputId": "89e4c886-0016-4dfe-a59a-9912a67474a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1077
        }
      },
      "source": [
        "\n",
        "def init_mean1(shape, dtype=None, name=None):\n",
        "    value = np.array([0,0,-1.0/4,0,-1.0/4,  0,0,0,-1.0/4,0,  0,0,0,0,0,  0,0,0,0,0,  -1.0/4,0,0,0,0 ])\n",
        "    value = np.reshape(value, shape)\n",
        "    return value\n",
        "\n",
        "def init_mean2(shape, dtype=None, name=None):\n",
        "    value = np.array([-1.0/4,-1.0/4,0,0,0,  0,0,0,0,0,  0,0,-1.0/4,0,0,  0,-1.0/4,0,0,0,  0,0,0,0,0 ])\n",
        "    value = np.reshape(value, shape)\n",
        "    return value\n",
        "\n",
        "def rand_rotate_matrix(angle1=-90, angle2=90, s1=0.5, s2=1.5):\n",
        "    random.random()\n",
        "    agx = random.uniform(angle1, angle2)  # do not use randint\n",
        "    agy = random.uniform(angle1, angle2)\n",
        "    s = random.uniform(s1, s2)\n",
        "    agx = math.radians(agx)\n",
        "    agy = math.radians(agy)\n",
        "    Rx = np.asarray([[1,0,0], [0,math.cos(agx),math.sin(agx)], [0, -math.sin(agx),math.cos(agx)]])\n",
        "    Ry = np.asarray([[math.cos(agy), 0, -math.sin(agy)], [0,1,0], [math.sin(agy), 0, math.cos(agy)]])\n",
        "    Ss = np.asarray([[s,0,0],[0,s,0],[0,0,s]])\n",
        "    # value = np.dot(Ry,np.dot(Rx,Ss))\n",
        "    value = np.dot(Ry,Ry)\n",
        "    # value = np.reshape(value, shape)\n",
        "    value=tf.convert_to_tensor(value)\n",
        "    # value = torch.from_numpy(value)\n",
        "    return value\n",
        "\n",
        "def rotate_skeleton_given_angle(skeleton, angle):\n",
        "    # angle = (angle_x, angle_y, angle_z ), radian\n",
        "    # notice: rotation matrix is a little different from definition above\n",
        "    assert(skeleton.shape[-1] ==3 )\n",
        "    org_shape = skeleton.shape\n",
        "    agx = angle[0]\n",
        "    Rx = np.asarray([[1,0,0], [0,math.cos(agx),-math.sin(agx)], [0, math.sin(agx),math.cos(agx)]])\n",
        "    agy = angle[1]\n",
        "    Ry = np.asarray([[math.cos(agy), 0, -math.sin(agy)], [0,1,0], [math.sin(agy), 0, math.cos(agy)]])\n",
        "    agz = angle[2]\n",
        "    Rz = np.asarray([[math.cos(agz), math.sin(agz), 0], [-math.sin(agz), math.cos(agz), 0], [0, 0, 1] ])\n",
        "    value = np.dot(Rz,np.dot(Ry,Rx))\n",
        "    skeleton0 = np.dot(skeleton.reshape((-1,3)),  value)\n",
        "    return skeleton0.reshape(org_shape)\n",
        "\n",
        "def rotate_skeleton_given_coord(skeleton, new_coord):\n",
        "    # angle = (angle_x, angle_y, angle_z ), radian\n",
        "    # notice: rotation matrix is a little different from definition above\n",
        "    assert(skeleton.shape[-1] ==3 )\n",
        "    org_shape = skeleton.shape\n",
        "    # notice: transform is necessary\n",
        "    skeleton0 = np.dot(skeleton.reshape((-1,3)),  new_coord.T )\n",
        "    return skeleton0.reshape(org_shape)\n",
        "\n",
        "def rand_rotate_matrix_symbol(angle=90, ss=0.5):\n",
        "    srs = T.shared_randomstreams.RandomStreams()\n",
        "    # np.pi / 180 *\n",
        "    agx =  (srs.uniform()*(2*angle) - angle)*np.pi/180\n",
        "    agy =  (srs.uniform()*(2*angle) - angle)*np.pi/180\n",
        "    # s = srs.uniform() + ss\n",
        "    Rx = T.stack(1,0,0, 0,T.cos(agx), T.sin(agx), 0, -T.sin(agx),T.cos(agx)).reshape((3,3))\n",
        "    Ry = T.stack(T.cos(agy), 0, -T.sin(agy),  0,1,0,  T.sin(agy), 0, T.cos(agy)).reshape((3,3))\n",
        "    # Ss = T.stack(s,0,0, 0,s,0, 0,0,s).reshape((3,3))\n",
        "    # value = theano.dot(Ry, theano.dot(Rx, Ss))\n",
        "    value = theano.dot(Ry, Rx)\n",
        "    return value\n",
        "\n",
        "class TransformLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TransformLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        new_x=tf.convert_to_tensor(x)\n",
        "        new_y=tf.convert_to_tensor(rand_rotate_matrix())\n",
        "        return K.in_train_phase(K.dot(x,tf.cast(rand_rotate_matrix(),tf.float32) ), x, training=training)\n",
        "        # return K.in_train_phase(T.concatenate([K.dot(x[:,:,:,0:3], rand_rotate_matrix_symbol()), x[:,:,:,3:6] ], axis=3), x, training=training)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "class construct_model(object):\n",
        "   # def __init__(self, param, dim_point=3, num_joints=25, num_class=60):\n",
        "    def __init__(self, param, dim_point=3, num_joints=25, num_class=2):\n",
        "        self._param = param\n",
        "        self._dim_point = dim_point\n",
        "        self._num_joints = num_joints\n",
        "        self._num_class = num_class\n",
        "\n",
        "    def group_person_list(self, list_file):\n",
        "        name_list = [line.strip() for line in open(list_file, 'r').readlines()]\n",
        "        vdname_list = [line[0:line.index('.skeleton')] for line in name_list ]\n",
        "        label_list = [(int(name[17:20])-1) for name in name_list]\n",
        "        idx_per = []\n",
        "        group_list = []\n",
        "        for idx, name in enumerate(name_list):\n",
        "            vdname = vdname_list[idx]\n",
        "            if idx == len(name_list)-1:\n",
        "                last_vdname = ''\n",
        "            else:\n",
        "                last_vdname = vdname_list[idx+1]\n",
        "            if vdname != last_vdname:\n",
        "                idx_per.append(idx)\n",
        "                # there exist samples with 3 skeletons, to check standard deviation\n",
        "                # print (len(idx_per), idx_per) 1 [10] 1 [11] 2 [12, 13]\n",
        "                group_list.append(idx_per)\n",
        "                # print [label_list[temp] for temp in idx_per]\n",
        "                idx_per = []\n",
        "            else:\n",
        "                idx_per.append(idx)\n",
        "        # print(group_list) [[0]-[2315]]\n",
        "        return group_list\n",
        "\n",
        "    def spatial_diff(self, skeleton):\n",
        "        assert(skeleton.shape[2] == 3), ' input must be skeleton array'\n",
        "        fidx = [1, 1, 21, 3,    21, 5,6,7,   21,9,10,11,   1,13,14,15,   1, 17,18,19,  2,8,8,12,12 ]\n",
        "        assert(len(fidx) == skeleton.shape[1] )\n",
        "        return skeleton[:,np.array(fidx)-1 ] - skeleton\n",
        "        # return np.concatenate((skeleton, skeleton[:,np.array(fidx)-1 ] - skeleton ), axis=-1)\n",
        "\n",
        "    def spatial_cross(self, skeleton):\n",
        "        assert(skeleton.shape[2] == 3), ' input must be skeleton array'\n",
        "        fidx1 = [17,21,4,21,  6,5,6,22,  21,11,12,24,  1,13,16,14,  18,17,18,19,  5,8,8,  12,12]\n",
        "        fidx2 = [13,1,21,3,  21,7,8,23,  10,9,10,25,  14,15,14,15,  1,19,20,18,  9,23,22, 25,24]\n",
        "        skt1 = skeleton[:,np.array(fidx1)-1 ] - skeleton\n",
        "        skt2 = skeleton[:,np.array(fidx2)-1 ] - skeleton\n",
        "        return 100*np.cross(skt1, skt2)\n",
        "\n",
        "    def load_sample_one_skeleton(self, h5_file, list_file, num_seq=100, ovr_num=100, spatil_diff=True ):\n",
        "        '''\n",
        "        To change overlap number\n",
        "        '''\n",
        "        name_list = [line.strip() for line in open(list_file, 'r').readlines()]\n",
        "        vdname_list = [line[0:line.index('.skeleton')] for line in name_list ]\n",
        "        label_list = [(int(name[17:20])-1) for name in name_list] # 为什么要-1？？ 0-59  fall是42！！\n",
        "        new_label_list=[]\n",
        "        fall_number=0\n",
        "        fall_number2=0\n",
        "        for i in name_list:\n",
        "            if \"A043\" in i:\n",
        "                fall_number2=fall_number2+1\n",
        "\n",
        "        for label in label_list:\n",
        "            if label == 42:\n",
        "                new_label_list.append(1)\n",
        "                fall_number=fall_number+1\n",
        "            else:\n",
        "                new_label_list.append(0)\n",
        "\n",
        "        print(\"load fall number\",fall_number)\n",
        "        print(\"load fall number from file\",fall_number2)\n",
        "        print(\"load total number\",len(name_list))\n",
        "        # print(label_list) # [32, 5, 42, 2, 3, 30, 39, 36, 42, 18, 20, 42, 35, 35,\n",
        "        # print(vdname_list) # 'S006C002P007R001A033', 'S006C001P007R002A006', 'S002C003P007R001A043', 'S012C002P037R002A003', 'S002C001P011R002A004', 'S007C001P007R002A031', 'S007C003P007R002A040', 'S005C001P010R001A037', 'S008C001P032R002A043', 'S014C002P039R002A019', 'S004C001P007R001A021', 'S006C002P007R002A043', 'S016C003P040R002A036', 'S016C003P040R002A036',\n",
        "\n",
        "        # read angles\n",
        "        # angle_list = [line.strip() for line in open(angle_file, 'r').readlines()]\n",
        "        # angle_list_array = np.array([map(float, line.split()) for line in angle_list] )\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "        vid_list = []\n",
        "        with h5py.File(h5_file,'r') as hf:\n",
        "            group_list = self.group_person_list(list_file)\n",
        "            for idx_per in group_list:\n",
        "                # labels in list are the same\n",
        "                label_per = new_label_list[idx_per[0]]\n",
        "                vdname = vdname_list[idx_per[0]]\n",
        "                for idx in idx_per:\n",
        "                    skeleton = np.asarray(hf.get(name_list[idx]))\n",
        "\n",
        "                    if spatil_diff:\n",
        "                        #skeleton = self.spatial_diff(skeleton)\n",
        "                        skeleton = self.spatial_cross(skeleton)\n",
        "\n",
        "                    # print(skeleton.shape) #(60, 25, 3) (73, 25, 3) (60, 25, 3) (103, 25, 3) (96, 25, 3) (71, 25, 3) (48, 25, 3) (118, 25, 3) (97, 25, 3) (68, 25, 3) (84, 25, 3) (72, 25, 3) (68, 25, 3) (68, 25, 3) (63, 25, 3)\n",
        "                    if skeleton.shape[0] > num_seq:\n",
        "                        start = 0\n",
        "                        while start + num_seq < skeleton.shape[0]:\n",
        "                            X.append(skeleton[start:start+num_seq])  # 0-100 100-200\n",
        "                            Y.append(label_per)\n",
        "                            vid_list.append(vdname)\n",
        "                            start = start + ovr_num\n",
        "                            # print((skeleton[start:start+num_seq]).shape) (3, 25, 3) (18, 25, 3)\n",
        "                        X.append(skeleton[-num_seq:]) # 为什么？？\n",
        "                        # print((skeleton[-num_seq:]).shape) (100, 25, 3) (100, 25, 3)\n",
        "                        Y.append(label_per)\n",
        "                        vid_list.append(vdname)\n",
        "                    else:\n",
        "                        skeleton = np.concatenate((np.zeros((num_seq-skeleton.shape[0], skeleton.shape[1], skeleton.shape[2])), skeleton), axis=0)\n",
        "                        # print(skeleton.shape) (100, 25, 3) 补零\n",
        "                        X.append(skeleton)\n",
        "                        Y.append(label_per)\n",
        "                        vid_list.append(vdname)\n",
        "        X = np.asarray(X).astype(np.float32)\n",
        "        Y = (np.asarray(Y)).astype(np.int32)\n",
        "        return X, Y, vid_list\n",
        "\n",
        "    def base_model(self, sub_mean=False, rotate=True):\n",
        "        '''\n",
        "        use stacked two layers as baseline, use stacked three layers later\n",
        "        K.learning_phase()\n",
        "        assert(self._dim_point == 3)\n",
        "        data = Dense(self._dim_point, kernel_initializer=rand_rotate_matrix, trainable=False)(skt_input)\n",
        "        '''\n",
        "        skt_input = Input(shape=(self._param['num_seq'], self._num_joints, self._dim_point) ) # To fix length of sequence\n",
        "        data = skt_input\n",
        "        if rotate:\n",
        "            if self._dim_point == 3:\n",
        "                data = TransformLayer()(skt_input)\n",
        "            else:\n",
        "                data = Reshape((self._param['num_seq'], int(self._num_joints*self._dim_point/3),3))(skt_input)\n",
        "                data = TransformLayer()(data)\n",
        "                data = Reshape((self._param['num_seq'], self._num_joints, self._dim_point))(data)\n",
        "\n",
        "        if sub_mean:\n",
        "            data = Permute((1,3,2))(data)\n",
        "            data2 = Dense(1, kernel_initializer=init_mean1, trainable=False)(data)\n",
        "            data2 = Lambda(lambda x:K.repeat_elements(x, self._num_joints, axis=-1),\n",
        "                           output_shape=lambda s: (s[0], s[1], s[2], s[3]*self._num_joints))(data2)\n",
        "            data = Add()([data, data2] )\n",
        "\n",
        "        # make sure do not subtract two mean vectors and concatenate the results\n",
        "        data = Reshape((self._param['num_seq'], self._num_joints*self._dim_point))(data)\n",
        "\n",
        "        data = SpatialDropout1D(0.05)(data)\n",
        "        out = Bidirectional(LSTM(512, return_sequences=True))(data)\n",
        "        out = SpatialDropout1D(0.05)(out)\n",
        "        out = Bidirectional(LSTM(512, return_sequences=True))(out)\n",
        "        out = SpatialDropout1D(0.05)(out)\n",
        "        out = Bidirectional(LSTM(512, return_sequences=True))(out)\n",
        "        # 把T.max改成了 K.max\n",
        "        out = Lambda(lambda x:K.max(x, axis=1), output_shape=lambda s: (s[0], s[2]))(out)\n",
        "        out = Dropout(0.5)(out)\n",
        "        out = Activation('relu')(out)\n",
        "        prob = Dense(self._num_class, activation='softmax')(out)\n",
        "\n",
        "        model = Model(skt_input, prob)\n",
        "        opt = SGD(lr=self._param['base_learn_rate'], decay=self._param['weight_regular'], momentum=0.9, nesterov=True)\n",
        "        model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "\n",
        "    def train_model(self):\n",
        "        model = self.base_model()\n",
        "\n",
        "        # test\n",
        "        valX, valY, val_vid_list = self.load_sample_one_skeleton(self._param['tst_arr_file'], self._param['tst_lst_file'],\n",
        "                                                                 self._param['num_seq'] ) # self._param['tst_angle_file'],\n",
        "        # train\n",
        "        trainX, trainY, train_vid_list = self.load_sample_one_skeleton(self._param['trn_arr_file'], self._param['trn_lst_file'],\n",
        "                                                                       self._param['num_seq'] )\n",
        "        test_fall=0\n",
        "        train_fall=0\n",
        "        for i in valY:\n",
        "            if i==1:\n",
        "                test_fall=test_fall+1\n",
        "\n",
        "        for j in trainY:\n",
        "             if j==0:\n",
        "                 train_fall=train_fall+1\n",
        "\n",
        "        trainY = np_utils.to_categorical(trainY, self._num_class )\n",
        "        valY = np_utils.to_categorical(valY, self._num_class )\n",
        "\n",
        "\n",
        "        print( 'train data:', trainX.shape, trainY.shape)\n",
        "        print( 'test data:', valX.shape, valY.shape)\n",
        "        print(\"train fall data\", train_fall)\n",
        "        print(\"test fall data\", test_fall)\n",
        "\n",
        "        def save_hdf5(model, fileName):\n",
        "            fid = h5py.File(fileName,'w')\n",
        "            weight = model.get_weights()\n",
        "            for i in range(len(weight)):\n",
        "                fid.create_dataset('weight'+str(i),data=weight[i])\n",
        "            fid.close()\n",
        "\n",
        "        def read_hdf5(model, fileName):\n",
        "            fid=h5py.File(fileName,'r')\n",
        "            weight = []\n",
        "            for i in range(len(fid.keys())):\n",
        "                weight.append(fid['weight'+str(i)][:])\n",
        "            model.set_weights(weight)\n",
        "\n",
        "        def schedule(epoch):\n",
        "            lr = K.get_value(model.optimizer.lr)\n",
        "            if epoch % self._param['step_inter'] == 0 and epoch > 0:\n",
        "                lr = lr*self._param['lr_gamma']\n",
        "            return np.float(lr)\n",
        "\n",
        "        write_file = False\n",
        "        if self._param['write_file']:\n",
        "            write_file = True\n",
        "            fid_out = open(self._param['write_file_name'], 'w') #deep_bkp.txt\n",
        "\n",
        "        save_model = False\n",
        "        if self._param['save_model']:\n",
        "            save_model = True\n",
        "            save_path = self._param['save_path'] #'data/save_param_temp/deep_bkp'\n",
        "\n",
        "        if self._param['initial_file'] != None:\n",
        "            read_hdf5(model, self._param['initial_file'] )\n",
        "\n",
        "        class evaluateVal(keras.callbacks.Callback):\n",
        "            def __init__(self, vid_list):\n",
        "                self.group_list, self.gt_val = self.merge_list(vid_list)\n",
        "\n",
        "            def merge_list(self, vid_list):\n",
        "                group_list = []\n",
        "                gt_val = []\n",
        "                idx_per = []\n",
        "                for idx, name in enumerate(vid_list):\n",
        "                    if idx == len(vid_list)-1:\n",
        "                        last_name = ''\n",
        "                    else:\n",
        "                        last_name = vid_list[idx+1]\n",
        "                    if name != last_name:\n",
        "                        idx_per.append(idx)\n",
        "                        gt_val.append(np.argmax(valY[idx]) )\n",
        "                        group_list.append(np.asarray(idx_per) )\n",
        "                        idx_per = []\n",
        "                    else:\n",
        "                        idx_per.append(idx)\n",
        "                return group_list, gt_val\n",
        "\n",
        "            def on_epoch_end(self, epoch, logs={}):\n",
        "                #if ((epoch) % 2==0):\n",
        "                if 1:\n",
        "                   # val_loss = model.evaluate(valX, valY, batch_size=512, verbose=0)[0]\n",
        "                    prob_val = model.predict(valX, batch_size=512, verbose=0)\n",
        "                    pred = np.asarray([np.argmax(np.mean(prob_val[idx], axis=0)) for idx in self.group_list ] )\n",
        "                    acc = sum( int(pred[i]) == self.gt_val[i] for i in range(len(self.gt_val))) / float(len(self.gt_val))\n",
        "                    #train_loss = model.evaluate(trainX, trainY, batch_size=512, verbose=0)[0]\n",
        "                    #cmd_str1 = 'evluation epoch=%d, learn_rate=%f, train loss=%f, validation loss=%f, validation accuracy=%f' % (epoch,\n",
        "                    #K.get_value(model.optimizer.lr), train_loss, val_loss, acc)\n",
        "                    cmd_str = 'evluation epoch=%d, learn_rate=%f, validation accuracy=%f' % (epoch, K.get_value(model.optimizer.lr), acc)\n",
        "                    print( cmd_str)\n",
        "                    #print( cmd_str1)\n",
        "                    # if 'fid_out' in locals() or 'fid_out' in globals():\n",
        "                    if write_file:\n",
        "                        fid_out.write(cmd_str + '\\n')\n",
        "                    if (epoch % 4==0) and epoch > 0 and save_model:\n",
        "                        save_file = save_path + ('_epoch%d.h5' % epoch) #'data/save_param_temp/deep_bkp'\n",
        "                        if os.path.exists(save_file):\n",
        "                            os.remove(save_file)\n",
        "                        # model.save_weights(save_file)\n",
        "                        save_hdf5(model, save_file)\n",
        "\n",
        "        reduce_lr = LearningRateScheduler(schedule)\n",
        "        # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=0.001)\n",
        "\n",
        "        evaluate_val = evaluateVal(val_vid_list)\n",
        "        model.fit(trainX, trainY, batch_size=self._param['batchsize'], epochs=self._param['max_iter'],\n",
        "                  callbacks=[evaluate_val, reduce_lr ], shuffle=True, verbose=1)\n",
        "def run_model():\n",
        "    param = {}\n",
        "    param['max_iter'] = 150\n",
        "    param['step_inter'] = 40\n",
        "    param['base_learn_rate'] = 0.02 #  defaults 0.02\n",
        "    # param['base_learn_rate'] = 0.001250 # finetune learning rate\n",
        "    param['lr_gamma'] = 0.5\n",
        "    param['weight_regular'] = 0\n",
        "    param['batchsize'] = 128 # previous 64\n",
        "    # for multi-scale model, 512 output of memory\n",
        "    param['num_seq'] = 100\n",
        "\n",
        "    if 0:\n",
        "        param['trn_arr_file'] = 'drive/Colab Notebooks/data/view_seq_1_20/new_array_list_train.h5'\n",
        "        param['trn_lst_file'] = 'drive/Colab Notebooks/data/view_seq_1_20/new_file_list_train.txt'\n",
        "        # param['trn_angle_file'] = '../data/view_seq/new_angle_list_train.txt'\n",
        "        param['tst_arr_file'] = 'drive/Colab Notebooks/data/view_seq_1_20/new_array_list_test.h5'\n",
        "        param['tst_lst_file'] = 'drive/Colab Notebooks/data/view_seq_1_20/new_file_list_test.txt'\n",
        "        # param['tst_angle_file'] = '../data/view_seq/new_angle_list_test.txt'\n",
        "    else:\n",
        "        param['trn_arr_file'] = 'drive/Colab Notebooks/data/subj_seq_1_20/new_array_list_train.h5'\n",
        "        param['trn_lst_file'] = 'drive/Colab Notebooks/data/subj_seq_1_20/new_file_list_train.txt'\n",
        "        param['tst_arr_file'] = 'drive/Colab Notebooks/data/subj_seq_1_20/new_array_list_test.h5'\n",
        "        # param['trn_angle_file'] = '../data/subj_seq/new_angle_list_train.txt'\n",
        "        param['tst_lst_file'] = 'drive/Colab Notebooks/data/subj_seq_1_20/new_file_list_test.txt'\n",
        "        # param['tst_angle_file'] = '../data/subj_seq/new_angle_list_test.txt'\n",
        "\n",
        "    param['write_file'] = True\n",
        "    param['write_file_name'] = 'subj.txt' # 'subj.txt', 'view.txt'\n",
        "    param['save_model'] = True\n",
        "    param['save_path'] = 'drive/Colab Notebooks/data/model_temp_1_20_CS_spatial_cross2/subj'\n",
        "    param['initial_file'] = None\n",
        "\n",
        "    model = construct_model(param)\n",
        "    model.train_model()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    run_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 100, 25, 3)        0         \n",
            "_________________________________________________________________\n",
            "transform_layer_2 (Transform (None, 100, 25, 3)        0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 100, 75)           0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_4 (Spatial (None, 100, 75)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 100, 1024)         2408448   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_5 (Spatial (None, 100, 1024)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 100, 1024)         6295552   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_6 (Spatial (None, 100, 1024)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 100, 1024)         6295552   \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 15,001,602\n",
            "Trainable params: 15,001,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "load fall number 289\n",
            "load fall number from file 289\n",
            "load total number 1256\n",
            "load fall number 709\n",
            "load fall number from file 709\n",
            "load total number 3091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5ee4b0daeba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-5ee4b0daeba6>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5ee4b0daeba6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         trainX, trainY, train_vid_list = self.load_sample_one_skeleton(self._param['trn_arr_file'], self._param['trn_lst_file'],\n\u001b[0;32m--> 250\u001b[0;31m                                                                        self._param['num_seq'] )\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mtest_fall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mtrain_fall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5ee4b0daeba6>\u001b[0m in \u001b[0;36mload_sample_one_skeleton\u001b[0;34m(self, h5_file, list_file, num_seq, ovr_num, spatil_diff)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mvid_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mgroup_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_person_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx_per\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}